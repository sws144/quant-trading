{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1d9e183",
   "metadata": {},
   "source": [
    "#  ## E: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c96570",
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for na pipeline\n",
    "import warnings\n",
    "import sklearn\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.base import TransformerMixin  # for custom transformers\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ccabda",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "## read in data\n",
    "\n",
    "df_XY = pd.read_csv(\"output/c_resulttradewattr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b447cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  get_feature_names function\n",
    "# https://johaupt.github.io/scikit-learn/tutorial/python/data%20processing/ml%20pipeline/model%20interpretation/columnTransformer_feature_names.html\n",
    "def get_feature_names(column_transformer):\n",
    "    \"\"\"Get feature names from all transformers.\n",
    "    Returns\n",
    "    -------\n",
    "    feature_names : list of strings\n",
    "        Names of the features produced by transform.\n",
    "    \"\"\"\n",
    "    # Remove the internal helper function\n",
    "    # check_is_fitted(column_transformer)\n",
    "\n",
    "    # Turn loopkup into function for better handling with pipeline later\n",
    "    def get_names(trans):\n",
    "        # >> Original get_feature_names() method\n",
    "        if trans == \"drop\" or (hasattr(column, \"__len__\") and not len(column)):\n",
    "            return []\n",
    "        if trans == \"passthrough\":\n",
    "            if hasattr(column_transformer, \"_df_columns\"):\n",
    "                if (not isinstance(column, slice)) and all(\n",
    "                    isinstance(col, str) for col in column\n",
    "                ):\n",
    "                    return column\n",
    "                else:\n",
    "                    return column_transformer._df_columns[column]\n",
    "            else:\n",
    "                indices = np.arange(column_transformer._n_features)\n",
    "                return [i for i in indices[column]]\n",
    "        if not hasattr(trans, \"get_feature_names\"):\n",
    "            # >>> Change: Return input column names if no method avaiable\n",
    "            # Turn error into a warning\n",
    "            warnings.warn(\n",
    "                \"Transformer %s (type %s) does not \"\n",
    "                \"provide get_feature_names. \"\n",
    "                \"Will return input column names if available\"\n",
    "                % (str(name), type(trans).__name__)\n",
    "            )\n",
    "            # For transformers without a get_features_names method, use the input\n",
    "            # names to the column transformer\n",
    "            if column is None:\n",
    "                return []\n",
    "            else:\n",
    "                return [f for f in column]\n",
    "\n",
    "        return [f for f in trans.get_feature_names()]\n",
    "\n",
    "    ### Start of processing\n",
    "    feature_names = []\n",
    "\n",
    "    # Allow transformers to be pipelines. Pipeline steps are named differently, so preprocessing is needed\n",
    "    if type(column_transformer) == sklearn.pipeline.Pipeline:\n",
    "        l_transformers = [\n",
    "            (name, trans, None) for step, name, trans in column_transformer._iter()\n",
    "        ]\n",
    "    else:\n",
    "        # For column transformers, follow the original method\n",
    "        l_transformers = column_transformer.transformers_\n",
    "\n",
    "    for name, trans, column in l_transformers:\n",
    "        if type(trans) == sklearn.pipeline.Pipeline:\n",
    "            # Recursive call on pipeline\n",
    "            _names = get_feature_names(trans)\n",
    "            # if pipeline has no transformer that returns names\n",
    "            if len(_names) == 0:\n",
    "                _names = [f for f in column]\n",
    "            feature_names.extend(_names)\n",
    "        else:\n",
    "            feature_names.extend(get_names(trans))\n",
    "\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3331ab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom transformers\n",
    "class Numerizer(TransformerMixin):\n",
    "    import pandas as pd\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        Y = X.apply(pd.to_numeric, errors=\"coerce\")\n",
    "        return Y\n",
    "\n",
    "\n",
    "class StringTransformer(TransformerMixin):\n",
    "    import pandas as pd\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        Y = pd.DataFrame(X).astype(\"string\")\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54add40d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "## create na pipeline\n",
    "\n",
    "# update columns headers to clean up\n",
    "df_XY.columns = list(\n",
    "    pd.Series(df_XY.columns)\n",
    "    .astype(str)\n",
    "    .str.replace(\" \", \"_\", regex=True)\n",
    "    .str.upper()\n",
    "    .str.strip()\n",
    "    .str.replace(\"/\", \"_\")\n",
    "    .str.replace(\"*\", \"_\")\n",
    ")\n",
    "\n",
    "# start with numeric, utilizng explore data before\n",
    "numeric_features = df_XY.select_dtypes(include=np.number).columns.tolist()\n",
    "numeric_features = numeric_features + [\n",
    "    \"%_TO_STOP\",\n",
    "    \"%_TO_TARGET\",\n",
    "    \"GROWTH_0.5TO0.75\",\n",
    "    \"ROIC_(BW_ROA_ROE)\",\n",
    "    \"TGT_FWD_P_E\",\n",
    "    \"YEARS_TO_NORMALIZATION\",\n",
    "]\n",
    "numeric_features = list(set(numeric_features))\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"numerizer\", Numerizer()),\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=-1)),\n",
    "    ]\n",
    ")\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"_NA_\")),\n",
    "        (\"stringtransformer\", StringTransformer()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# numerical\n",
    "\n",
    "# categorical_features = ['embarked', 'sex', 'pclass']\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "categorical_features = list(set(df_XY.columns).difference(set(numeric_features)))\n",
    "\n",
    "preprocessor_na = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    # remainder = 'passthrough' # not needed anymore\n",
    ")\n",
    "\n",
    "XY_imputed = preprocessor_na.fit_transform(df_XY)\n",
    "\n",
    "columns = get_feature_names(preprocessor_na)\n",
    "\n",
    "df_XY_imputed = pd.DataFrame(XY_imputed, columns=columns).convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa62f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target\n",
    "\n",
    "df_XY_imputed[\"PCT_RET_FINAL\"] = df_XY_imputed[\"PNL\"] / (\n",
    "    df_XY_imputed[\"OPEN_PRICE\"] * df_XY_imputed[\"QUANTITY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f75aa8b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# TODO create moving avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179bfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final columns\n",
    "\n",
    "print(df_XY_imputed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b9084",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check no na's left in numerical\n",
    "\n",
    "try:\n",
    "    assert (\n",
    "        df_XY_imputed[numeric_features].isna().sum().sum() == 0\n",
    "    ), \"NAs remain in numerical\"\n",
    "except:\n",
    "    print(\"NAs remain in numerical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d620b42a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "## import api spec\n",
    "\n",
    "import yaml\n",
    "from yaml import Loader\n",
    "\n",
    "with open(\"data-tests/_apispecs.yaml\") as f:\n",
    "    api_specs = yaml.load(f, Loader=Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6570cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## validate based on api spec\n",
    "\n",
    "from openapi_schema_validator import validate\n",
    "import json\n",
    "\n",
    "schema = api_specs[\"components\"][\"schemas\"][\"Tradelog\"]\n",
    "\n",
    "json_str = df_XY_imputed.to_json(orient=\"records\")\n",
    "json_test = json.loads(json_str)\n",
    "\n",
    "i = 0\n",
    "for row in json_test:\n",
    "    try:\n",
    "        validate(row, schema)\n",
    "    except:\n",
    "        print(f\"failed on {i}th row \")\n",
    "        break\n",
    "    i = i + 1\n",
    "\n",
    "print(\"validation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f92a7e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "## save api spec to html\n",
    "\n",
    "import os\n",
    "\n",
    "# feed yaml file to swagger python, then create api.html\n",
    "os.system(\n",
    "    \"python swagger_yaml_to_html.py < data-tests/_apispecs.yaml > templates/api.html\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01a7cf1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "## save results\n",
    "\n",
    "df_XY_imputed.to_csv(\"output/e_resultcleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d949fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save imputer\n",
    "\n",
    "dump(preprocessor_na, \"output/e_preprocessor_na.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a95beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "auto:percent,ipynb",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "p1analyzetrades",
   "language": "python",
   "name": "p1analyzetrades"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
